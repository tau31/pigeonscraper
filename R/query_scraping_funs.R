# wrapper functions for scrapping ----

# Build css query table for years and organizations -----

#' Build css query table for years and organizations
#'
#' Connects to a remote selenium driver and scrapes the APRU website to
#' build a query table used to scrape individual race information.
#'
#' @return tibble with year and organization info and corresponding css queries.
#'
#' @import RSelenium

pigeon_query_builder <-
  function() {
    remDr <- connect_remDr()
    page_source <- get_page_source(
      remDr = remDr,
      link = "https://pigeon-ndb.com/races/")
    years <- extract_years(parsed_html = page_source)
    css_query_tbl <- extract_orgs(years, remDr = remDr)
    remDr$close()
    return(css_query_tbl)
  }



# Scrape pigeon data -----

#' Scrapes race tale using css query table from
#' \code{\link{pigeon_query_builder}}
#'
#' Takes in a remote driver selenium driver and scrapes the APRU website to
#' using the a css query for year and organization. Parses race tables for every
#' race organized by an association within a year.
#'
#' @param css_query_entry Entry from css query tble generated by
#' \code{\link{pigeon_query_builder}}
#'
#' @return list of tibble with race information and race results tables.
#'
#' @import RSelenium

scraper <-
  function(css_query_entry) {
    remDr <- connect_remDr()
    remDr$open(silent = TRUE)
    remDr_go_to_link(remDr = remDr, link = "https://pigeon-ndb.com/races/")

    Sys.sleep(2)
    # Extract race html options
    race_html <- extract_race_html_options(css_query_tbl = css_query_entry,
                                           remDr = remDr)
    # parse htlm page with tables into a list of xml documents
    xml_doc <- race_table_parse(race_xml_nodeset = race_html, remDr= remDr)

    # Extract tables into tibbles and assemble race_results and race_info tbls
    raw_tbls <- assemble_tbl(races_xml = xml_doc,css_query_tbl = css_query_entry)

    # Pre-process tables
    tbls_list <- pre_process_tbls(raw_tbls)
    remDr$close()
    return(tbls_list)
  }

# pigeon scraper function --------------

#' Scraping function to extract data from the APRU National Database. It uses
#' functionality from the \code{furrr} package to run multiple queries in
#' parallel.
#'
#' This is a do all function. It creates a query table, scrapes and processes
#' data from the APRU website in parallel. It save the output in
#' \code{data/raw_data} in single \code{.rds} files by group of queries.
#'
#' @param group_len Number of organization queries to be run by core
#' @param n_workers Number of parallel workers
#'
#' @importFrom future multiprocess

pigeon_scraper <-
  function(group_len, n_workers) {

    cat("Checking if path data/raw_data exists
        if not, one will be created \n")
    if(dir.exists(here::here("data", "raw_data")) == FALSE) {
      dir.create(here::here("data", "raw_data"), recursive = TRUE)
    }

    start_chrome_remDr(kill = FALSE)

    cat("building css query \n")
    css_query_tbl <- pigeon_query_builder()

    cat("creating start-end indices \n")
    start_end <- create_start_end(group_len, nrow(css_query_tbl))

    start <- start_end[,1]
    end <- start_end[,2]

    cat("preparing parallel processing \n")
    plan_future(n_workers)
    cat("scraping function \n")

    temp_l <- furrr::future_map(
      1:nrow(start_end),
      function(i){
        temp_race_data  <- purrr::map(
          start[i] : end[i],
          purrr::safely(function(g) {
            scraper(css_query_tbl[g, ])
          }))
        saveRDS(temp_race_data, file = here::here(
          "data",
          "raw_data",
          paste("tbl_", start[i], "_", end[i], ".rds", sep = "")
        )
        )
      }
    )
  }
